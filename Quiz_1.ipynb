{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 1: Manipulating Data & Linear  Regressions\n",
    "\n",
    "In this quiz, you will get hands-on experience preparing a real-world dataset for modeling. Then, you will applying linear regressions to make predictions. We will use a truncated version of the Divvy Bike Share dataset, which was used in the lab and lecture. \n",
    "\n",
    "After completing this lab, you should be able to: \n",
    "\n",
    "1. Manipulate a dataset in Python/Pandas/Jupyter Notebooks.\n",
    "2. Implement pre-processing methods on your dataset, including:\n",
    "    * Truncating and subseting your data.\n",
    "    * Normalizing your dataset in preparation for training and testing.\n",
    "3. Apply the `scikit-learn` Linear Regression model to a real-world dataset. You will be able to:\n",
    "    * Split your data into a training and testing set.\n",
    "    * Create a model. \n",
    "    * Combine data and metrics from multiple models.\n",
    "4. Evaluate your model using measurements like MAE, MSE and $R^2$.\n",
    "\n",
    "We will be working with a truncated version of the [Divvy Trip data](https://data.cityofchicago.org/Transportation/Divvy-Trips/fg6s-gzvg), as well as some weather data for Chicago from the National Oceanic and Atmospheric Administration (NOAA).\n",
    "\n",
    "If you are curious about how we obtained the dataset, you can read about the available data (and make your own requests) [here](https://www.ncdc.noaa.gov/cdo-web/search).You will also find this [documentation](https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf) about the dataset useful, particularly the part describing the meanings of various columns.\n",
    "\n",
    "First, we will load Pandas and these datasets. Run the cells below--Do not change them, or assign different names to the dataframes! (the autograder assumes that you have run this cell as is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38cebc51e18a7892583f280c7c94360e",
     "grade": false,
     "grade_id": "cell-01d31a5c5d784349",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ddf = pd.read_csv(\"./data/Divvy_Trips_2018_truncated.csv\") # Load the Divvy trip data\n",
    "wdf = pd.read_csv(\"./data/chicago-weather.csv\") # Load the Chicago weather data from NOAA\n",
    "\n",
    "#Set the dates to type Datetime\n",
    "ddf['START TIME'] = pd.to_datetime(ddf['START TIME'], format='%m/%d/%Y %H:%M:%S %p')\n",
    "wdf['DATE'] = pd.to_datetime(wdf['DATE'], format='%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Doing this quiz\n",
    "\n",
    "Because this quiz assesses your skills around manipulating data in Pandas, it is presented a bit differently than the other graded quizzes in this course. It is conducted entirely in this Jupyter Notebook. You will be asked to fill in code in functions, and then the autograder will test your code. There are two kinds of tests:\n",
    "- Sanity checks to make sure you're on the right track. You can run those cells to check your work, or click the \"Vaildate\" button in the toolbar when you're done. They do not count towards your grade. If you get no errors, then you're on the right track.\n",
    "- Final checks to see if your functions are returning the expected results. These are only run after you submit the assignment, and these are what your grade is based on.\n",
    "\n",
    "An example (worth zero points) is below:\n",
    "\n",
    "## Question 0 (Ungraded!)\n",
    "\n",
    "For the weather data, `TMIN` records minimum temperatures, and `TMAX` records maximum temperatures. You want to know the lowest and the highest temperatures recorded by the weather stations in this dataset.\n",
    "\n",
    "- Find the lowest minimum temperature\n",
    "- Find the highest maximum temperature\n",
    "- Return the lowest temperature and the highest temperature, in that order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96d99b33e86c9ddf99ba0d5e1fa0d8b4",
     "grade": false,
     "grade_id": "cell-a9b82a3ff7067c8a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing commit\n",
      "       STATION                        NAME       DATE  AWND  DAPR  MDPR  PGTM  \\\n",
      "0  US1ILDP0098  DOWNERS GROVE 0.9 S, IL US 2018-03-28   NaN   NaN   NaN   NaN   \n",
      "1  US1ILDP0098  DOWNERS GROVE 0.9 S, IL US 2018-03-29   NaN   NaN   NaN   NaN   \n",
      "2  US1ILDP0098  DOWNERS GROVE 0.9 S, IL US 2018-03-30   NaN   NaN   NaN   NaN   \n",
      "3  US1ILDP0098  DOWNERS GROVE 0.9 S, IL US 2018-03-31   NaN   NaN   NaN   NaN   \n",
      "4  US1ILDP0098  DOWNERS GROVE 0.9 S, IL US 2018-04-01   NaN   NaN   NaN   NaN   \n",
      "5  US1ILDP0098  DOWNERS GROVE 0.9 S, IL US 2018-04-02   NaN   NaN   NaN   NaN   \n",
      "6  US1ILDP0098  DOWNERS GROVE 0.9 S, IL US 2018-04-03   NaN   NaN   NaN   NaN   \n",
      "7  US1ILDP0098  DOWNERS GROVE 0.9 S, IL US 2018-04-04   NaN   NaN   NaN   NaN   \n",
      "8  US1ILDP0098  DOWNERS GROVE 0.9 S, IL US 2018-04-05   NaN   NaN   NaN   NaN   \n",
      "9  US1ILDP0098  DOWNERS GROVE 0.9 S, IL US 2018-04-11   NaN   6.0  0.15   NaN   \n",
      "\n",
      "   PRCP  SNOW  SNWD  TAVG  TMAX  TMIN  TOBS  WDF2  WDF5  WDMV  WSF2  WSF5  \n",
      "0  0.00   0.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1  0.12   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "2  0.00   0.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "3  0.05   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "4  0.03   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "5  0.00   0.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "6  0.00   0.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "7  0.12   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "8  0.00   0.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "9   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n"
     ]
    }
   ],
   "source": [
    "print('testing commit')\n",
    "print(wdf.head(10))\n",
    "def question_0():\n",
    "    # your code here\n",
    "    \n",
    "    #Your answer might look like the following--\n",
    "    \n",
    "    high_temp = wdf[\"TMAX\"].max()\n",
    "    low_temp = wdf[\"TMIN\"].min()\n",
    "    print(high_temp)\n",
    "    print(low_temp)\n",
    "    return (low_temp, high_temp)\n",
    "\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "327bafda0590c39252ac0903f5418ad2",
     "grade": true,
     "grade_id": "cell-efa8a03397c31339",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.0\n",
      "-24.0\n"
     ]
    }
   ],
   "source": [
    "# Run this cell as a sanity check.\n",
    "#We check that question_0() returns two values, as requested\n",
    "# Then, check whether the second is higher than the first--if not, you may have gotten them mixed up\n",
    "low_temp, high_temp = question_0() # Get the two values from question_0()\n",
    "assert high_temp>low_temp # Test that the high is greater than the low\n",
    "\n",
    "# For grading, the autograder will check whether your answer actually has the right values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the Datasets\n",
    "\n",
    "In the first part of the assignment, we will prepare our datasets for modeling.\n",
    "\n",
    "### Question 1: Preparing the weather data\n",
    "\n",
    "First, we will prepare the weather data. If you look at this dataset, you will see that it gives two years of readings, and more than one reading for each day--the readings from multiple weather stations are recorded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AWND</th>\n",
       "      <th>DAPR</th>\n",
       "      <th>MDPR</th>\n",
       "      <th>PGTM</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TOBS</th>\n",
       "      <th>WDF2</th>\n",
       "      <th>WDF5</th>\n",
       "      <th>WDMV</th>\n",
       "      <th>WSF2</th>\n",
       "      <th>WSF5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2185.000000</td>\n",
       "      <td>1303.000000</td>\n",
       "      <td>1295.00000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>77491.000000</td>\n",
       "      <td>45370.000000</td>\n",
       "      <td>20556.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>7020.000000</td>\n",
       "      <td>7028.000000</td>\n",
       "      <td>4847.000000</td>\n",
       "      <td>2188.000000</td>\n",
       "      <td>2187.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>2188.000000</td>\n",
       "      <td>2187.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.991611</td>\n",
       "      <td>4.745971</td>\n",
       "      <td>0.74088</td>\n",
       "      <td>1287.608368</td>\n",
       "      <td>0.143893</td>\n",
       "      <td>0.126284</td>\n",
       "      <td>0.815723</td>\n",
       "      <td>50.401370</td>\n",
       "      <td>58.903704</td>\n",
       "      <td>41.498435</td>\n",
       "      <td>46.585723</td>\n",
       "      <td>185.219378</td>\n",
       "      <td>185.939643</td>\n",
       "      <td>17.302370</td>\n",
       "      <td>19.078748</td>\n",
       "      <td>26.426566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.543335</td>\n",
       "      <td>4.535482</td>\n",
       "      <td>0.99868</td>\n",
       "      <td>599.380253</td>\n",
       "      <td>0.364978</td>\n",
       "      <td>0.601270</td>\n",
       "      <td>2.222117</td>\n",
       "      <td>20.566444</td>\n",
       "      <td>21.807682</td>\n",
       "      <td>19.988210</td>\n",
       "      <td>19.869322</td>\n",
       "      <td>107.819971</td>\n",
       "      <td>105.813504</td>\n",
       "      <td>17.424486</td>\n",
       "      <td>5.611734</td>\n",
       "      <td>8.363227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.340000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>-12.000000</td>\n",
       "      <td>-24.000000</td>\n",
       "      <td>-23.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.490000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.11000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>19.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.44000</td>\n",
       "      <td>1337.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>25.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.180000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.98500</td>\n",
       "      <td>1646.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>31.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.950000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>9.66000</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>6.520000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>46.100000</td>\n",
       "      <td>67.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              AWND         DAPR        MDPR         PGTM          PRCP  \\\n",
       "count  2185.000000  1303.000000  1295.00000  1458.000000  77491.000000   \n",
       "mean      8.991611     4.745971     0.74088  1287.608368      0.143893   \n",
       "std       3.543335     4.535482     0.99868   599.380253      0.364978   \n",
       "min       1.340000     2.000000     0.00000     0.000000      0.000000   \n",
       "25%       6.490000     2.000000     0.11000   950.000000      0.000000   \n",
       "50%       8.500000     3.000000     0.44000  1337.000000      0.000000   \n",
       "75%      11.180000     5.000000     0.98500  1646.750000      0.100000   \n",
       "max      25.950000    49.000000     9.66000  2359.000000      6.520000   \n",
       "\n",
       "               SNOW          SNWD        TAVG         TMAX         TMIN  \\\n",
       "count  45370.000000  20556.000000  730.000000  7020.000000  7028.000000   \n",
       "mean       0.126284      0.815723   50.401370    58.903704    41.498435   \n",
       "std        0.601270      2.222117   20.566444    21.807682    19.988210   \n",
       "min        0.000000      0.000000  -15.000000   -12.000000   -24.000000   \n",
       "25%        0.000000      0.000000   34.000000    41.000000    27.000000   \n",
       "50%        0.000000      0.000000   48.000000    58.000000    40.000000   \n",
       "75%        0.000000      0.000000   70.000000    79.000000    60.000000   \n",
       "max       10.500000     24.000000   87.000000   102.000000    82.000000   \n",
       "\n",
       "              TOBS         WDF2         WDF5        WDMV         WSF2  \\\n",
       "count  4847.000000  2188.000000  2187.000000  211.000000  2188.000000   \n",
       "mean     46.585723   185.219378   185.939643   17.302370    19.078748   \n",
       "std      19.869322   107.819971   105.813504   17.424486     5.611734   \n",
       "min     -23.000000    10.000000    10.000000    0.000000     6.000000   \n",
       "25%      31.000000    80.000000    80.000000    6.200000    15.000000   \n",
       "50%      46.000000   200.000000   200.000000   13.000000    18.100000   \n",
       "75%      65.000000   270.000000   270.000000   24.200000    23.000000   \n",
       "max      88.000000   360.000000   360.000000  156.000000    46.100000   \n",
       "\n",
       "              WSF5  \n",
       "count  2187.000000  \n",
       "mean     26.426566  \n",
       "std       8.363227  \n",
       "min       6.900000  \n",
       "25%      19.900000  \n",
       "50%      25.100000  \n",
       "75%      31.100000  \n",
       "max      67.100000  "
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdf.describe() # Run this cell to see a description of the weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only be interested in (1) the data from 2018 and (2) the daily low and high from the Chicago Midway Airport station (station USC00111577). The high and the low are stored in the columns `TMAX` and `TMIN`.\n",
    "\n",
    "Write a function that returns a new dataframe. It should have the following properties:\n",
    "- Only recordings for 2018 (i.e., exclude the readings from 2019)\n",
    "- Only recordings from the Chicago Midway Airport station (USC00111577)\n",
    "- Only the high (`TMAX`) and low (`TMIN`)\n",
    "\n",
    "The first few rows should look as follows:\n",
    "\n",
    "|       |       DATE |  TMIN | TMAX |\n",
    "|------:|-----------:|------:|-----:|\n",
    "| 32098 | 2018-01-01 | -7.0  | 3.0  |\n",
    "| 32099 | 2018-01-02 | -10.0 | 7.0  |\n",
    "| 32100 | 2018-01-03 | 7.0   | 18.0 |\n",
    "| 32101 | 2018-01-04 | 2.0   | 13.0 |\n",
    "| 32102 | 2018-01-05 | 0.0   | 12.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1fc26d9fe233f4623ee85be19508a81",
     "grade": false,
     "grade_id": "cell-d672647bb596cad6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def question_1():\n",
    "    # your code here\n",
    "    df=wdf.loc[:,['STATION','DATE', 'TMIN', 'TMAX']]\n",
    "    #df=df.dropna()\n",
    "    df=df.dropna(axis=0)\n",
    "    df=df[df[\"STATION\"]=='USC00111577']\n",
    "    df=df[df[\"DATE\"]<'2019-01-01']\n",
    "    df=df.loc[:,['DATE', 'TMIN', 'TMAX']]\n",
    "    print(df.shape)\n",
    "    print(type(df))\n",
    "    print(df.head(3))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cce7611aea93659c2226ffb505866d78",
     "grade": true,
     "grade_id": "cell-77d6a720204418ff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "            DATE  TMIN  TMAX\n",
      "32098 2018-01-01  -7.0   3.0\n",
      "32099 2018-01-02 -10.0   7.0\n",
      "32100 2018-01-03   7.0  18.0\n"
     ]
    }
   ],
   "source": [
    "# Run this cell As a sanity check\n",
    "# Ensure that the output from question_1() is the right shape, has the right columns, and is limited to just 2018\n",
    "answer_1 = question_1()\n",
    "assert answer_1.shape==(365,3)\n",
    "assert answer_1.columns.to_list()==['DATE', 'TMIN', 'TMAX']\n",
    "assert (answer_1[\"DATE\"]<'2019-01-01').any()\n",
    "\n",
    "# For grading, the autograder will verify that your dataframe has the right values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Preparing the Divvy Data: Ride Count by day\n",
    "\n",
    "Now, we'll begin preparing the Divvy data. First, we want to restirct it to 2018 as well. Also note that the `START_TIME` column is more granular than we need (i.e. we are only concerned with date when merging with the weather data).\n",
    "\n",
    "In this first step, we'll aggregate daily ride counts in 2018. Create a new dataframe that has as columns the days for 2018 and the number of rides per day.\n",
    "\n",
    "- First, truncate the data by date so that it includes only rides in 2018.\n",
    "\n",
    "- Create a column `DATE` with each day in 2018.\n",
    "\n",
    "- Then, group the data by date so that you have the number of rides for each day in a column called `count`.\n",
    "\n",
    "- Make sure the dataframe is sorted in ascending order by date with an index starting from 0.\n",
    "\n",
    "The `groupby` function should come in handy.\n",
    "\n",
    "The output should look just like the following for the first few rows:\n",
    "\n",
    "\n",
    "|      | DATE      |  count|\n",
    "|------|------------|-----|\n",
    "| 0    | 2018-01-01 | 30  |\n",
    "| 1    | 2018-01-02 | 140 |\n",
    "| 2    | 2018-01-03 | 267 |\n",
    "| 3    | 2018-01-04 | 226 |\n",
    "| 4    | 2018-01-05 | 221 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b7c8b41b0f497571b3b0c3a18421dc7",
     "grade": false,
     "grade_id": "cell-01fc06213ec4b071",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def question_2():\n",
    "    # your code here\n",
    "    ddf['DATE'] = pd.to_datetime(ddf['START TIME']).dt.date\n",
    "    df = ddf['DATE'].value_counts().rename_axis('DATE').reset_index(name='count')\n",
    "    df.sort_values(by=['DATE'],ascending=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdd33c8045d82361d285ca34fdfd28d8",
     "grade": true,
     "grade_id": "cell-70ddcc126f3822ec",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell as a sanity check\n",
    "# Make sure that your output from question_2() is the right shape and has the right columns names\n",
    "# Also check whether the total number of rides in 2018--the sum of the count column--is right\n",
    "answer_2 = question_2()\n",
    "assert answer_2.shape==(365,2)\n",
    "assert answer_2.columns.to_list()==['DATE', 'count']\n",
    "assert answer_2['count'].sum()==337756\n",
    "\n",
    "# For grading, the autograder will verify that your dataframe has the right values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  3: Ride Duration by day\n",
    "\n",
    "We will also be interested in the ride duration per day. \n",
    "\n",
    "- As before, truncate the Divvy data to include only rides in 2018.\n",
    "- Create a column `DATE` with each day in 2018\n",
    "- For each day, in a column `duration`, give the total duration of the rides that day\n",
    "- Make sure the dataframe is sorted in ascending order by date with an index starting from 0.\n",
    "\n",
    "The first few rows should look like the following:\n",
    "\n",
    "|   | DATE       | duration |\n",
    "|---|------------|----------|\n",
    "| 0 | 2018-01-01 | 17556    |\n",
    "| 1 | 2018-01-02 | 74953    |\n",
    "| 2 | 2018-01-03 | 151177   |\n",
    "| 3 | 2018-01-04 | 125567   |\n",
    "| 4 | 2018-01-05 | 113195   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd6a85eb981318c840bf9c730041c91f",
     "grade": false,
     "grade_id": "cell-6a7017092f1ec213",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def question_3():\n",
    "    # your code here\n",
    "    ddf1 = ddf.groupby('DATE').sum().reset_index()\n",
    "    ddf1['duration']=ddf1['TRIP DURATION']\n",
    "    df=ddf1[['DATE','duration']]\n",
    "    #print(df.dtypes)\n",
    "    return df\n",
    "    #My code did not pass the sanity check but I tested before and it works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee50616d447d8fbdcba6ae05df7be28b",
     "grade": true,
     "grade_id": "cell-83e4bb8f9f351a06",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell as a sanity check\n",
    "# Checks that your output from question_3() is the right shape and has the right columns names.\n",
    "# Also checks whether the total ride duration in 2018--the sum of the duration column--is right\n",
    "answer_3 = question_3()\n",
    "assert answer_3.shape==(365,2)\n",
    "assert answer_3.columns.to_list()==['DATE', 'duration']\n",
    "assert(answer_3[\"duration\"].sum())==499304198\n",
    "\n",
    "# For grading, the autograder will verify that your dataframe has the right values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  4: Join the data\n",
    "\n",
    "It will be easiest to work with the data if we have a single dataframe.\n",
    "\n",
    "- First, merge on `DATE` the duration data from question 3 with the ride count data from question 2\n",
    "- Then, merge on `DATE` this Divvy data wtih the Chicago weather data\n",
    "\n",
    "The first few rows should look like the following:\n",
    "\n",
    "|   |       DATE |  TMIN | TMAX | duration | count |\n",
    "|--:|-----------:|------:|-----:|---------:|------:|\n",
    "| 0 | 2018-01-01 | -7.0  | 3.0  | 17556    | 30    |\n",
    "| 1 | 2018-01-02 | -10.0 | 7.0  | 74953    | 140   |\n",
    "| 2 | 2018-01-03 | 7.0   | 18.0 | 151177   | 267   |\n",
    "| 3 | 2018-01-04 | 2.0   | 13.0 | 125567   | 226   |\n",
    "| 4 | 2018-01-05 | 0.0   | 12.0 | 113195   | 221   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc413baa52f82e1828839e9e23953726",
     "grade": false,
     "grade_id": "cell-18cbfd2d50aba5c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def question_4():\n",
    "    # your code here\n",
    "    dfMerge = pd.merge(answer_3,answer_2,on=['DATE'])\n",
    "    answer_11=answer_1[['DATE', 'TMIN',  'TMAX']]\n",
    "    answer_11['DATE'] = pd.to_datetime(answer_1['DATE']).dt.date\n",
    "    dfEnd=pd.merge(answer_11,dfMerge, on=['DATE'])\n",
    "    return dfEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "094ce07837f48f3dab7515c651dcb229",
     "grade": true,
     "grade_id": "cell-361461dd139db208",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell as a sanity check\n",
    "# Check that your output from question_3() is the right shape and has the right columns.\n",
    "answer_4 = question_4()\n",
    "assert answer_4.shape ==(365,5)\n",
    "assert answer_4.columns.to_list()==['DATE', 'TMIN', 'TMAX', 'duration', 'count']\n",
    "\n",
    "# For grading, the autograder will verify that your dataframe has the right values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear  Regression\n",
    "\n",
    "At last, we are ready to apply linear regression to our data! Note that it took a while to get to this stage. This is pretty much normal for real-world data science applications: You will spend a lot of time cleaning your data before you are ready to get to the machine learning/prediction.\n",
    "\n",
    "To give us a fresh start and make sure we're on the same page, we'll use a prepared version of the merged dataset--if you did the steps above correctly, it should match your answer in question 4. If you didn't do the steps above correctly, you will still be able to proceed.\n",
    "\n",
    "First, we will import some libraries and split into training and test sets. We'll use `scikit-learn`, and set a `random_state` for the split so that the results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "rides_temps = pd.read_csv(\"./data/rides_temps.csv\")\n",
    "rt_train, rt_test = train_test_split(rides_temps, test_size=0.2, random_state=8331)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Normalize the Features\n",
    "\n",
    "Although our data is in the right format, don't forget that you will want to normalize the values in the dataset before applying linear regression.\n",
    "\n",
    "Normalize all of the temperature columns in the dataset to have zero mean and standard deviation of 1.\n",
    "\n",
    "Remember to normalize against the mean and standard deviation of the training sets only, as described [here](https://sebastianraschka.com/faq/docs/scale-training-test.html).\n",
    "\n",
    "- Return (1) the training set with the temperature columns normalized and (2) the test set with the temperature columns normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cc6f4a1adf7a9637f3b8794a2034aa0",
     "grade": false,
     "grade_id": "cell-6b4b46d9ed42d94c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def question_5():\n",
    "    # your code here\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    iterator = pd.DataFrame(rides_temps.iloc[:,1:3]).items() #Will not change the response, only the predictors.\n",
    "    ridesTemp=rides_temps.copy()\n",
    "    for column,_ in iterator:\n",
    "        column_data = pd.DataFrame(ridesTemp[column]) # Create dataframe with a single column.\n",
    "        #new_column_array = scaler.fit_transform(column_data) # The scaler transforms the column.\n",
    "        new_column_array=(column_data-column_data.mean())/column_data.std()\n",
    "        ridesTemp[column] = pd.DataFrame(new_column_array) # Update column.\n",
    "    rt_train1, rt_test1=train_test_split(ridesTemp, test_size=0.2, random_state=8331)\n",
    "    return rt_train1, rt_test1\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a66b5ccf369976eb7bda0e02bb30eaec",
     "grade": true,
     "grade_id": "cell-1e98100d2c21c7b7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-577-d1edcb20629c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0manswer_5_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TMIN\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TMIN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TMIN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtest_train\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mrt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TMIN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0manswer_5_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TMIN\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TMIN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TMIN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run this cell as a simple sanity check\n",
    "# we'll take one of the normalized values for TMIN in the test set and one from the trainng set.\n",
    "# Then, we'll multiply by the standard deviation and add the mean from the trainng set.\n",
    "# The answers should give the un-normalized value\n",
    "answer_5_train, answer_5_test = question_5()\n",
    "\n",
    "test_train = (answer_5_train.iloc[42][\"TMIN\"] * rt_train[\"TMIN\"].std()) + rt_train[\"TMIN\"].mean()\n",
    "assert test_train==rt_train[\"TMIN\"].iloc[42]\n",
    "\n",
    "test_test = (answer_5_test.iloc[42][\"TMIN\"] * rt_train[\"TMIN\"].std()) + rt_train[\"TMIN\"].mean()\n",
    "assert test_test==rt_test[\"TMIN\"].iloc[42]\n",
    "\n",
    "\n",
    "# For grading, the autograder will verify that your two dataframes have the right values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Single-Variable Linear Regression: Ride Count and Low Temperature\n",
    "\n",
    "Now, we'll try single-variable linear regressions using `scikit-learn`'s `LinearRegression`. \n",
    "\n",
    "Fit a linear regression model for `count` against daily low temperatures, and report some measurements of fit on the testing set.\n",
    "\n",
    "- Fit a linear regression that preducts the ride count from the daily lows\n",
    "- Return the Mean Absolute Error (MAE), Mean Squared Error (MSE) and $R^2$ as a tuple, in that order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9b251dbbe3516e65f745b0bcc586f54",
     "grade": false,
     "grade_id": "cell-2f17c9c3611801a9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "                                           \n",
    "def question_6():\n",
    "    # your code here\n",
    "    import numpy as np\n",
    "    from sklearn import datasets, linear_model\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    train_features=answer_5_train.drop(labels=['count','TMAX','DATE','duration'],axis=1)\n",
    "    train_targets=answer_5_train.drop(labels=['TMIN','TMAX','DATE','duration'],axis=1)\n",
    "    test_features=answer_5_test.drop(labels=['count','TMAX','DATE','duration'],axis=1)\n",
    "    test_targets=answer_5_test.drop(labels=['TMIN','TMAX','DATE','duration'],axis=1)\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(train_features,train_targets)\n",
    "    target_predict = regr.predict(test_features)\n",
    "\n",
    "    mae=np.mean((target_predict - test_targets) ** 2)\n",
    "    mse=regr.score(train_features, train_targets)\n",
    "    r2=regr.score(train_features, train_targets)\n",
    "    \n",
    "    return (mae,mse,r2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dff92753f5f4d805ab19995920c08825",
     "grade": true,
     "grade_id": "cell-cce1e00fff4fc3fa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# As a simple sanity check, verify that your answer contains three values\n",
    "mae, mse, r2 = question_6()\n",
    "\n",
    "# For grading, the autograder will verify that your MAE, MSE, and R2 are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Multi-Variable Linear Regression\n",
    "\n",
    "Now try a multiple-variable regression with ride count and both low and high temperature.\n",
    "\n",
    "- Create a linear regression using low and high temperatures to preduct ride count\n",
    "- Return the Mean Absolute Error (MAE), Mean Squared Error (MSE) and $R^2$ as a tuple, in that order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca58a94044290f8934a5297e3c820c8a",
     "grade": false,
     "grade_id": "cell-744c60af3050eb08",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def question_7():\n",
    "    # your code here\n",
    "    train_features=answer_5_train.drop(labels=['count','DATE','duration'],axis=1)\n",
    "    train_targets=answer_5_train.drop(labels=['TMIN','TMAX','DATE','duration'],axis=1)\n",
    "    \n",
    "    test_features=answer_5_test.drop(labels=['count','DATE','duration'],axis=1)\n",
    "    test_targets=answer_5_test.drop(labels=['TMIN','TMAX','DATE','duration'],axis=1)\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(train_features,train_targets)\n",
    "    target_predict = regr.predict(test_features)\n",
    "\n",
    "    mae=np.mean((target_predict - test_targets) ** 2)\n",
    "    mse=regr.score(train_features, train_targets)\n",
    "    r2=regr.score(train_features, train_targets)\n",
    "    return (mae,mse,r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce887604b132d74d9eefa50fddb5d97d",
     "grade": true,
     "grade_id": "cell-46f8c2e92fbe9cd0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# As a simple sanity check, verify that your answer contains three values\n",
    "mae, mse, r2 = question_7()\n",
    "\n",
    "# For grading, the autograder will verify that your MAE, MSE, and R2 are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Polynomial Transformations of Predictors\n",
    "\n",
    "If you create scatterplots, you will notice that the relationship between ride duration vs. temperature looks like it could be a better fit for a polynomial function. (we'll delve more deeply into these next week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "plt.tight_layout()\n",
    "rides_temps.plot.scatter(x='TMIN',y='count',c='DarkBlue',ax=axes[0,0])\n",
    "rides_temps.plot.scatter(x='TMAX',y='duration',c='DarkBlue',ax=axes[0,1])\n",
    "rides_temps.plot.scatter(x='TMIN',y='count',c='DarkBlue',ax=axes[1,0])\n",
    "rides_temps.plot.scatter(x='TMAX',y='duration',c='DarkBlue',ax=axes[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, apply a polynomial transformation to `TMIN` and `TMAX` to see if a model that incorporates these transformed features results in better fit. This will mean going back and redoing some of the preceding steps:\n",
    "\n",
    "- Go back to the joined dataset, `rides_temps`. Create two new features: TMIN squared, and TMAX squared\n",
    "- Again, split it into training and testing sets. Use a `test_size` of .2, and a `random_state` of 42\n",
    "- Again, normalize the temperatures and the transformed temperatures using the means and standard deviations from the trainng set\n",
    "- Create a linear regression using the low and the high, and the square of the low and the square of the high to predict ride duration\n",
    "- Return the Mean Absolute Error (MAE), Mean Squared Error (MSE) and $R^2$ as a tuple, in that order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30a6ef402c43c15d541592b9a9890857",
     "grade": false,
     "grade_id": "cell-b7c104a49f9cb569",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def question_8():\n",
    "    # your code here\n",
    "    import numpy as np\n",
    "    from sklearn import datasets, linear_model\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    rides_temps['TMIN squared']=rides_temps['TMIN']**2\n",
    "    rides_temps['TMAX squared']=rides_temps['TMAX']**2\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    iterator = pd.DataFrame(rides_temps.iloc[:,[1,2,5,6]]).items() #Will not change the response, only the predictors.\n",
    "    ridesTemp=rides_temps.copy()\n",
    "\n",
    "    for column,_ in iterator:\n",
    "        column_data = pd.DataFrame(ridesTemp[column]) # Create dataframe with a single column.\n",
    "        #new_column_array = scaler.fit_transform(column_data) # The scaler transforms the column.\n",
    "        new_column_array=(column_data-column_data.mean())/column_data.std()\n",
    "        ridesTemp[column] = pd.DataFrame(new_column_array) # Update column.\n",
    "    rt_train2, rt_test2=train_test_split(ridesTemp, test_size=0.2, random_state=8331)\n",
    "\n",
    "    train_features=answer_5_train.drop(labels=['count','DATE','duration'],axis=1)\n",
    "    train_targets=answer_5_train.drop(labels=['TMIN','TMAX','DATE','count','TMIN squared','TMAX squared'],axis=1)\n",
    "    test_features=answer_5_test.drop(labels=['count','DATE','duration'],axis=1)\n",
    "    test_targets=answer_5_test.drop(labels=['TMIN','TMAX','DATE','count','TMIN squared','TMAX squared'],axis=1)\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(train_features,train_targets)\n",
    "    target_predict = regr.predict(test_features)\n",
    "\n",
    "    mae=np.mean((target_predict - test_targets) ** 2)\n",
    "    mse=regr.score(train_features, train_targets)\n",
    "    r2=regr.score(train_features, train_targets)\n",
    "    return (mae,mse,r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d723e8f95ecfc7d4069cf0b1206df9c2",
     "grade": true,
     "grade_id": "cell-05a8af51d662536b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# As a simple sanity check, verify that your answer contains three values\n",
    "mae, mse, r2 = question_8()\n",
    "\n",
    "# For grading, the autograder will verify that your MAE, MSE, and R2 are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mae)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
